{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree Name: Accounting\n",
      "Link: /future-students/undergraduate/explore-programs/accounting\n",
      "Degree Name: Actuarial Science\n",
      "Link: /future-students/undergraduate/explore-programs/actuarial-science\n",
      "Degree Name: Anthropology (Bachelor of Arts)\n",
      "Link: /future-students/undergraduate/explore-programs/anthropology-arts\n",
      "Degree Name: Anthropology (Bachelor of Science)\n",
      "Link: /future-students/undergraduate/explore-programs/anthropology-science\n",
      "Degree Name: Archaeology (Bachelor of Arts)\n",
      "Link: /future-students/undergraduate/explore-programs/archaeology-arts\n",
      "Degree Name: Archaeology (Bachelor of Science)\n",
      "Link: /future-students/undergraduate/explore-programs/archaeology-science\n",
      "Degree Name: Architecture\n",
      "Link: /future-students/undergraduate/explore-programs/architecture\n",
      "Degree Name: Art History\n",
      "Link: /future-students/undergraduate/explore-programs/art-history\n",
      "Degree Name: Astrophysics\n",
      "Link: /future-students/undergraduate/explore-programs/astrophysics\n",
      "Degree Name: Biochemistry\n",
      "Link: /future-students/undergraduate/explore-programs/biochemistry\n",
      "Degree Name: Bioinformatics\n",
      "Link: /future-students/undergraduate/explore-programs/bioinformatics\n",
      "Degree Name: Biological Sciences\n",
      "Link: /future-students/undergraduate/explore-programs/biological-sciences\n",
      "Degree Name: Biomechanics\n",
      "Link: /future-students/undergraduate/explore-programs/biomechanics\n",
      "Degree Name: Biomedical Engineering\n",
      "Link: /future-students/undergraduate/explore-programs/biomedical-engineering\n",
      "Degree Name: Biomedical Sciences\n",
      "Link: /future-students/undergraduate/explore-programs/biomedical-science\n",
      "Degree Name: \n",
      "Link: \n",
      "Degree Name: Current page1\n",
      "Link: ?block_id=97944&block_name=ucws_page_filter_tags_block&search_api_fulltext=%20&term_parent%5B0%5D=1258&&page=0\n",
      "Degree Name: Page2\n",
      "Link: ?block_id=97944&block_name=ucws_page_filter_tags_block&search_api_fulltext=%20&term_parent%5B0%5D=1258&&page=1\n",
      "Degree Name: Page3\n",
      "Link: ?block_id=97944&block_name=ucws_page_filter_tags_block&search_api_fulltext=%20&term_parent%5B0%5D=1258&&page=2\n",
      "Degree Name: \n",
      "Link: ?block_id=97944&block_name=ucws_page_filter_tags_block&search_api_fulltext=%20&term_parent%5B0%5D=1258&&page=1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = 'https://www.ucalgary.ca/future-students/undergraduate/programs'\n",
    "\n",
    "# Send a GET request to fetch the HTML content of the page\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Locate the container that holds the degree names and links\n",
    "container = soup.find('div', class_='views-element-container')\n",
    "\n",
    "# Find all degree items (assuming each degree name and link is in 'a' tags within the container)\n",
    "degree_items = container.find_all('a')\n",
    "\n",
    "# Loop through the items to extract the name and link of each degree\n",
    "for item in degree_items:\n",
    "    degree_name = item.get_text(strip=True)\n",
    "    degree_link = item['href']\n",
    "    \n",
    "    # Print degree name and link\n",
    "    print(f'Degree Name: {degree_name}')\n",
    "    print(f'Link: {degree_link}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree Name: Accounting, Link: /future-students/undergraduate/explore-programs/accounting\n",
      "Degree Name: Actuarial Science, Link: /future-students/undergraduate/explore-programs/actuarial-science\n",
      "Degree Name: Anthropology (Bachelor of Arts), Link: /future-students/undergraduate/explore-programs/anthropology-arts\n",
      "Degree Name: Anthropology (Bachelor of Science), Link: /future-students/undergraduate/explore-programs/anthropology-science\n",
      "Degree Name: Archaeology (Bachelor of Arts), Link: /future-students/undergraduate/explore-programs/archaeology-arts\n",
      "Degree Name: Archaeology (Bachelor of Science), Link: /future-students/undergraduate/explore-programs/archaeology-science\n",
      "Degree Name: Architecture, Link: /future-students/undergraduate/explore-programs/architecture\n",
      "Degree Name: Art History, Link: /future-students/undergraduate/explore-programs/art-history\n",
      "Degree Name: Astrophysics, Link: /future-students/undergraduate/explore-programs/astrophysics\n",
      "Degree Name: Biochemistry, Link: /future-students/undergraduate/explore-programs/biochemistry\n",
      "Degree Name: Bioinformatics, Link: /future-students/undergraduate/explore-programs/bioinformatics\n",
      "Degree Name: Biological Sciences, Link: /future-students/undergraduate/explore-programs/biological-sciences\n",
      "Degree Name: Biomechanics, Link: /future-students/undergraduate/explore-programs/biomechanics\n",
      "Degree Name: Biomedical Engineering, Link: /future-students/undergraduate/explore-programs/biomedical-engineering\n",
      "Degree Name: Biomedical Sciences, Link: /future-students/undergraduate/explore-programs/biomedical-science\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL to fetch\n",
    "url = 'https://www.ucalgary.ca/future-students/undergraduate/programs'\n",
    "\n",
    "# Send HTTP request\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # This will ensure a valid response or throw an exception\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Prepare to find all relevant 'a' tags within the specific class indicating a degree program\n",
    "degree_links = soup.find_all('a', class_='search-results__item__link')  # Adjust class based on actual HTML\n",
    "\n",
    "# Collect each degree link and its text\n",
    "degrees = [(link.text.strip(), link['href']) for link in degree_links]\n",
    "\n",
    "# Print all extracted degree names and links\n",
    "for name, link in degrees:\n",
    "    print(f\"Degree Name: {name}, Link: {link}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#content\n",
      "https://my.ucalgary.ca/\n",
      "https://office365.ucalgary.ca/\n",
      "https://www.ucalgary.ca/it/\n",
      "https://d2l.ucalgary.ca/\n",
      "https://www.ucalgary.ca/iriss/\n",
      "https://myfacilities.ucalgary.ca/\n",
      "https://www.ucalgary.ca/hr/\n",
      "https://library.ucalgary.ca/\n",
      "https://www.godinos.com/\n",
      "https://csprd.my.ucalgary.ca/psp/csprd/EMPLOYEE/SA/c/COMMUNITY_ACCESS.CLASS_SEARCH.GBL?public=yes/\n",
      "https://profiles.ucalgary.ca/\n",
      "https://conted.ucalgary.ca/\n",
      "https://careers.ucalgary.ca/\n",
      "https://www.calgarybookstore.ca/\n",
      "https://www.ucalgary.ca/activeliving/\n",
      "https://calendar.ucalgary.ca/\n",
      "https://www.ucalgary.ca/map/\n",
      "#\n",
      "https://cumming.ucalgary.ca/\n",
      "https://arts.ucalgary.ca/\n",
      "https://grad.ucalgary.ca/\n",
      "https://www.ucalgary.ca/knes/\n",
      "https://law.ucalgary.ca/\n",
      "https://nursing.ucalgary.ca/\n",
      "https://www.ucalgary.edu.qa/\n",
      "https://science.ucalgary.ca/\n",
      "https://fsw.ucalgary.ca/\n",
      "https://vet.ucalgary.ca/\n",
      "https://haskayne.ucalgary.ca/\n",
      "https://sapl.ucalgary.ca/\n",
      "https://spp.ucalgary.ca/\n",
      "https://schulich.ucalgary.ca/\n",
      "https://werklund.ucalgary.ca/\n",
      "https://www.ucalgary.ca\n",
      "/\n",
      "/future-students\n",
      "#\n",
      "#\n",
      "/future-students/undergraduate\n",
      "#\n",
      "#\n",
      "/future-students/undergraduate/programs\n",
      "/future-students/undergraduate/programs/selecting-degree\n",
      "#\n",
      "#\n",
      "/future-students/undergraduate/why-ucalgary\n",
      "/future-students/undergraduate/why-ucalgary/academic-life\n",
      "/future-students/undergraduate/why-ucalgary/student-experience\n",
      "/future-students/undergraduate/why-ucalgary/housing-food\n",
      "/future-students/undergraduate/why-ucalgary/life-in-calgary\n",
      "#\n",
      "#\n",
      "/future-students/undergraduate/tours-events\n",
      "https://discover.ucalgary.ca/portal/undergraduate_tours\n",
      "https://discover.ucalgary.ca/portal/undergraduate_events\n",
      "#\n",
      "#\n",
      "/future-students/undergraduate/fees-finances\n",
      "/registrar/finances/tuition-and-fees/undergraduate-student-cost-estimator\n",
      "#\n",
      "#\n",
      "/future-students/undergraduate/admissions\n",
      "/future-students/undergraduate/admissions/requirements\n",
      "/future-students/undergraduate/admissions/dates\n",
      "/future-students/undergraduate/admissions/how-to-apply\n",
      "/future-students/undergraduate/admissions/transcripts-documents\n",
      "/future-students/undergraduate/admissions/transfer-credit\n",
      "/future-students/undergraduate/admissions/application-status\n",
      "#\n",
      "#\n",
      "/future-students/undergraduate/contact\n",
      "/future-students/undergraduate/contact/recruiters\n",
      "/future-students/undergraduate/contact/guides-publications\n",
      "/future-students/undergraduate/information/indigenous-students\n",
      "/future-students/undergraduate/information/international-students\n",
      "/future-students/undergraduate/information/supporters\n",
      "/future-students/undergraduate/information/high-school-counsellors\n",
      "#\n",
      "#\n",
      "https://www.ucalgary.ca/future-students/graduate\n",
      "https://www.ucalgary.ca/future-students/graduate/explore-programs\n",
      "https://www.ucalgary.ca/future-students/graduate/contact\n",
      "https://www.ucalgary.ca/future-students/graduate/finance\n",
      "https://www.ucalgary.ca/future-students/graduate/student-life\n",
      "https://www.ucalgary.ca/future-students/graduate/apply\n",
      "https://www.ucalgary.ca/future-students/graduate/indigenous\n",
      "https://www.ucalgary.ca/future-students/graduate/international\n",
      "https://www.ucalgary.ca/future-students/graduate/understanding-graduate-studies\n",
      "#\n",
      "#\n",
      "/future-students/open-studies\n",
      "/future-students/open-studies/degree-holders-and-non-degree-holders\n",
      "/future-students/open-studies/visiting\n",
      "/future-students/open-studies/high-school-students\n",
      "/future-students/open-studies/university-entrance-program\n",
      "/future-students/open-studies/open-studies-dates\n",
      "#\n",
      "#\n",
      "/future-students/professional-and-continuing-education\n",
      "https://conted.ucalgary.ca/\n",
      "https://haskayne.ucalgary.ca/future-students/executive\n",
      "https://socialwork.ucalgary.ca/pd\n",
      "https://cumming.ucalgary.ca/cme\n",
      "https://discover.ucalgary.ca/register/undergradrequest\n",
      "/future-students/undergraduate/admissions/how-to-apply\n",
      "/future-students/undergraduate/customizing-your-degree\n",
      "/future-students/undergraduate/customizing-your-degree\n",
      "/future-students/undergraduate/requirements\n",
      "/future-students/undergraduate/explore-programs/accounting\n",
      "/future-students/undergraduate/explore-programs/actuarial-science\n",
      "/future-students/undergraduate/explore-programs/anthropology-arts\n",
      "/future-students/undergraduate/explore-programs/anthropology-science\n",
      "/future-students/undergraduate/explore-programs/archaeology-arts\n",
      "/future-students/undergraduate/explore-programs/archaeology-science\n",
      "/future-students/undergraduate/explore-programs/architecture\n",
      "/future-students/undergraduate/explore-programs/art-history\n",
      "/future-students/undergraduate/explore-programs/astrophysics\n",
      "/future-students/undergraduate/explore-programs/biochemistry\n",
      "/future-students/undergraduate/explore-programs/bioinformatics\n",
      "/future-students/undergraduate/explore-programs/biological-sciences\n",
      "/future-students/undergraduate/explore-programs/biomechanics\n",
      "/future-students/undergraduate/explore-programs/biomedical-engineering\n",
      "/future-students/undergraduate/explore-programs/biomedical-science\n",
      "\n",
      "?block_id=97944&block_name=ucws_page_filter_tags_block&search_api_fulltext=%20&term_parent%5B0%5D=1258&&page=0\n",
      "?block_id=97944&block_name=ucws_page_filter_tags_block&search_api_fulltext=%20&term_parent%5B0%5D=1258&&page=1\n",
      "?block_id=97944&block_name=ucws_page_filter_tags_block&search_api_fulltext=%20&term_parent%5B0%5D=1258&&page=2\n",
      "?block_id=97944&block_name=ucws_page_filter_tags_block&search_api_fulltext=%20&term_parent%5B0%5D=1258&&page=1\n",
      "https://ucalgary.ca/about/our-organization/faculties\n",
      "/node/352413\n",
      "/future-students/undergraduate/contact\n",
      "https://www.ucalgary.ca/future-students/undergraduate/recruiters\n",
      "https://www.ucalgary.ca/future-students/undergraduate/contact\n",
      "https://www.facebook.com/ucalgaryFS\n",
      "https://www.instagram.com/choose.ucalgary/?hl=e\n",
      "https://www.youtube.com/channel/UC1VSD9wJ_SBgJcPI5PljAHQ\n",
      "https://ucalgary.ca/startsomething\n",
      "https://www.facebook.com/universityofcalgary\n",
      "https://www.twitter.com/ucalgary\n",
      "https://www.linkedin.com/company/university-of-calgary\n",
      "https://www.instagram.com/ucalgary\n",
      "https://www.youtube.com/c/UCalgaryCa\n",
      "https://www.ucalgary.ca/website-terms-conditions\n",
      "https://www.ucalgary.ca/legal-services/university-policies-procedures/privacy-policy\n",
      "https://ucalgarysurvey.qualtrics.com/SE/?SID=SV_2lcAqNmIryzHpZ3\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL to fetch\n",
    "url = 'https://www.ucalgary.ca/future-students/undergraduate/programs'\n",
    "\n",
    "# Send HTTP request\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Raises an exception for 4XX or 5XX status codes\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all 'a' tags and extract href attributes\n",
    "links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "\n",
    "# Print all extracted links\n",
    "for link in links:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Runs Chrome in headless mode.\n",
    "\n",
    "# Path to your ChromeDriver\n",
    "service = Service(executable_path='path_to_chromedriver')\n",
    "\n",
    "# Initialize the driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# URL to fetch\n",
    "url = 'https://www.ucalgary.ca/future-students/undergraduate/programs'\n",
    "\n",
    "# Load the page\n",
    "driver.get(url)\n",
    "\n",
    "# Optional: simulate scrolling to load dynamic content\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(5)  # Wait for content to load\n",
    "\n",
    "# Now, use Selenium's methods to find elements\n",
    "degree_elements = driver.find_elements(By.CSS_SELECTOR, 'a.search-results__item__link')\n",
    "\n",
    "degrees = [(elem.text, elem.get_attribute('href')) for elem in degree_elements]\n",
    "\n",
    "# Print all extracted degree names and links\n",
    "for name, link in degrees:\n",
    "    a.append(name)\n",
    "    b.append(link)\n",
    "\n",
    "# Clean up\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]\n",
    "a=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Degree Name': a,\n",
    "    'Link': b\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('graduate_links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Provide the path to your ChromeDriver\n",
    "service = Service(executable_path=\"path_to_chromedriver\")\n",
    "\n",
    "# Initialize the driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# URL to fetch\n",
    "url = 'https://grad.ucalgary.ca/future-students/explore-programs'\n",
    "\n",
    "# Load the page\n",
    "driver.get(url)\n",
    "\n",
    "# Optional: Scroll to the bottom to load all dynamic content\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(5)  # Wait for the page to load content\n",
    "\n",
    "# Find all degree program elements\n",
    "program_elements = driver.find_elements(By.CSS_SELECTOR, \"div.result-content\")\n",
    "\n",
    "# Extract the degree name and link\n",
    "programs = []\n",
    "for program in program_elements:\n",
    "    degree_name = program.find_element(By.TAG_NAME, \"p\").text.strip()\n",
    "    program_link = program.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "    programs.append((degree_name, program_link))\n",
    "\n",
    "# Print the results\n",
    "for name, link in programs:\n",
    "    a.append(name)\n",
    "    b.append(link)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "# Setup Chrome options to run in headless mode\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Provide the path to ChromeDriver\n",
    "driver_path = \"path/to/chromedriver\"\n",
    "service = Service(driver_path)\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Load the webpage\n",
    "url = \"https://grad.ucalgary.ca/future-students/explore-programs\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to fully load\n",
    "time.sleep(5)\n",
    "\n",
    "# Scroll to the bottom to ensure all content is loaded\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(5)  # Extra wait for all elements to load\n",
    "\n",
    "# Find all program elements\n",
    "program_elements = driver.find_elements(By.CLASS_NAME, \"program\")\n",
    "\n",
    "# Extract program name and link\n",
    "program_data = []\n",
    "for element in program_elements:\n",
    "    try:\n",
    "        program_name = element.text.strip()\n",
    "        link = element.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "        program_data.append((program_name, link))\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data: {e}\")\n",
    "\n",
    "# Print the extracted data\n",
    "for program_name, link in program_data:\n",
    "    a.append(program_name)\n",
    "    b.append(link)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content successfully saved to 'extracted_content.txt'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = \"https://grad.ucalgary.ca/future-students/explore-programs/anthropology-ma-thesis\"\n",
    "\n",
    "# Send a GET request to fetch the HTML content of the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the main content section by its ID or class (as per the provided sample)\n",
    "    main_content = soup.find('main', id='content')\n",
    "\n",
    "    # Extract all the text from the main content\n",
    "    if main_content:\n",
    "        text_content = main_content.get_text(separator='\\n', strip=True)\n",
    "\n",
    "        # Save the text content to a file\n",
    "        with open(\"extracted_content.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_content)\n",
    "\n",
    "        print(\"Content successfully saved to 'extracted_content.txt'.\")\n",
    "    else:\n",
    "        print(\"Main content not found.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The phrase 'International students: December 15 application deadline' was found in the file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Open and read the text file\n",
    "with open('extracted_content.txt', 'r', encoding='utf-8') as file:\n",
    "    # Store the entire content in a variable\n",
    "    file_content = file.read()\n",
    "\n",
    "# Search for the phrase \"200 and 300-level\" in the content\n",
    "search_phrase = \"International students: December 15 application deadline\"\n",
    "\n",
    "# Check if the phrase exists in the file content\n",
    "if search_phrase in file_content:\n",
    "    print(f\"The phrase '{search_phrase}' was found in the file.\")\n",
    "else:\n",
    "    print(f\"The phrase '{search_phrase}' was NOT found in the file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open and read the text file\n",
    "with open('certificate_in_arts_body.txt', 'r', encoding='utf-8') as file:\n",
    "    # Store the entire content in a variable\n",
    "    file_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df =pd.read_csv('calgray_links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.iloc[82:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content successfully saved to 'Calgray_university/Philosophy.txt'.\n",
      "Content successfully saved to 'Calgray_university/Physics.txt'.\n",
      "Content successfully saved to 'Calgray_university/Planning.txt'.\n",
      "Content successfully saved to 'Calgray_university/Plant_Biology.txt'.\n",
      "Content successfully saved to 'Calgray_university/Political_Science.txt'.\n",
      "Content successfully saved to 'Calgray_university/Psychology_(Bachelor_of_Arts).txt'.\n",
      "Content successfully saved to 'Calgray_university/Psychology_(Bachelor_of_Science).txt'.\n",
      "Content successfully saved to 'Calgray_university/Real_Estate_Studies.txt'.\n",
      "Content successfully saved to 'Calgray_university/Risk_Management_and_Insurance.txt'.\n",
      "Content successfully saved to 'Calgray_university/Risk_Management:_Insurance_and_Finance.txt'.\n",
      "Content successfully saved to 'Calgray_university/Social_Work.txt'.\n",
      "Content successfully saved to 'Calgray_university/Sociology.txt'.\n",
      "Content successfully saved to 'Calgray_university/Software_Engineering.txt'.\n",
      "Content successfully saved to 'Calgray_university/Statistics.txt'.\n",
      "Content successfully saved to 'Calgray_university/Supply_Chain_Management.txt'.\n",
      "Content successfully saved to 'Calgray_university/Sustainable_Systems_Engineering.txt'.\n",
      "Content successfully saved to 'Calgray_university/Undeclared_(Arts).txt'.\n",
      "Content successfully saved to 'Calgray_university/Urban_Studies.txt'.\n",
      "Content successfully saved to 'Calgray_university/Veterinary_Medicine.txt'.\n",
      "Content successfully saved to 'Calgray_university/Visual_Studies.txt'.\n",
      "Content successfully saved to 'Calgray_university/Zoology.txt'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Create a directory to store the text files\n",
    "os.makedirs(\"Calgray_university_Courses\", exist_ok=True)\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    degree_name = row['Degree Name']\n",
    "    url = row['Link']\n",
    "\n",
    "    # Send a GET request to fetch the HTML content of the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the main content section by its ID or class\n",
    "        main_content = soup.find('div', id='main-content')\n",
    "\n",
    "        if main_content:\n",
    "            # Extract all text from the main content\n",
    "            text_content = main_content.get_text(separator='\\n', strip=True)\n",
    "\n",
    "            # Create a valid filename by replacing invalid characters\n",
    "            filename = f\"Calgray_university_Courses/{degree_name.replace('/', '_').replace(' ', '_')}.txt\"\n",
    "\n",
    "            # Save the link and text content to the file\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "                # Write the link at the top\n",
    "                file.write(f\"Link: {url}\\n\\n\")\n",
    "                # Write the extracted content below the link\n",
    "                file.write(text_content)\n",
    "\n",
    "            print(f\"Content successfully saved to '{filename}'.\")\n",
    "        else:\n",
    "            print(f\"Main content not found for {degree_name}.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage for {degree_name}. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Scraping page 26...\n",
      "Scraping page 27...\n",
      "Scraping page 28...\n",
      "Scraping page 29...\n",
      "Scraping page 30...\n",
      "Scraping page 31...\n",
      "Scraping page 32...\n",
      "Scraping page 33...\n",
      "Scraping page 34...\n",
      "Scraping page 35...\n",
      "Scraping page 36...\n",
      "Scraping page 37...\n",
      "Scraping page 38...\n",
      "Scraping page 39...\n",
      "Scraping page 40...\n",
      "Scraping page 41...\n",
      "Scraping page 42...\n",
      "Scraping page 43...\n",
      "Scraping page 44...\n",
      "Scraping page 45...\n",
      "Scraping page 46...\n",
      "Scraping page 47...\n",
      "Scraping page 48...\n",
      "Scraping page 49...\n",
      "Scraping page 50...\n",
      "Scraping page 51...\n",
      "Scraping page 52...\n",
      "Scraping page 53...\n",
      "Scraping page 54...\n",
      "Scraping page 55...\n",
      "Scraping page 56...\n",
      "Scraping page 57...\n",
      "Scraping page 58...\n",
      "Scraping page 59...\n",
      "Scraping page 60...\n",
      "Scraping page 61...\n",
      "Scraping page 62...\n",
      "Scraping page 63...\n",
      "Scraping page 64...\n",
      "Scraping page 65...\n",
      "Scraping page 66...\n",
      "Scraping page 67...\n",
      "Scraping page 68...\n",
      "Scraping page 69...\n",
      "Scraping page 70...\n",
      "Scraping page 71...\n",
      "Scraping page 72...\n",
      "Scraping page 73...\n",
      "Scraping page 74...\n",
      "Scraping page 75...\n",
      "Scraping page 76...\n",
      "Scraping page 77...\n",
      "Scraping page 78...\n",
      "Scraping page 79...\n",
      "Scraping page 80...\n",
      "Scraping page 81...\n",
      "Scraping page 82...\n",
      "Scraping page 83...\n",
      "Scraping page 84...\n",
      "Scraping page 85...\n",
      "Scraping page 86...\n",
      "Scraping page 87...\n",
      "Scraping page 88...\n",
      "Scraping page 89...\n",
      "Scraping page 90...\n",
      "Scraping page 91...\n",
      "Scraping page 92...\n",
      "Scraping page 93...\n",
      "Scraping page 94...\n",
      "Scraping page 95...\n",
      "Scraping page 96...\n",
      "Scraping page 97...\n",
      "Scraping page 98...\n",
      "Scraping page 99...\n",
      "Scraping page 100...\n",
      "Scraping page 101...\n",
      "Scraping page 102...\n",
      "Scraping page 103...\n",
      "Scraping page 104...\n",
      "Scraping page 105...\n",
      "Scraping page 106...\n",
      "Scraping page 107...\n",
      "Scraping page 108...\n",
      "Scraping page 109...\n",
      "Scraping page 110...\n",
      "Scraping page 111...\n",
      "Scraping page 112...\n",
      "Scraping page 113...\n",
      "Scraping page 114...\n",
      "Scraping page 115...\n",
      "Scraping page 116...\n",
      "Scraping page 117...\n",
      "Scraping page 118...\n",
      "Scraping page 119...\n",
      "Scraping page 120...\n",
      "Scraping page 121...\n",
      "Scraping page 122...\n",
      "Scraping page 123...\n",
      "Scraping page 124...\n",
      "Scraping page 125...\n",
      "Scraping page 126...\n",
      "Scraping page 127...\n",
      "Scraping page 128...\n",
      "Scraping page 129...\n",
      "Scraping page 130...\n",
      "Scraping page 131...\n",
      "Scraping page 132...\n",
      "Scraping page 133...\n",
      "Scraping page 134...\n",
      "Scraping page 135...\n",
      "Scraping page 136...\n",
      "Scraping page 137...\n",
      "Scraping page 138...\n",
      "Scraping page 139...\n",
      "Scraping page 140...\n",
      "Scraping page 141...\n",
      "Scraping page 142...\n",
      "Scraping page 143...\n",
      "Scraping page 144...\n",
      "Scraping page 145...\n",
      "Scraping page 146...\n",
      "Scraping page 147...\n",
      "Scraping page 148...\n",
      "Scraping page 149...\n",
      "Scraping page 150...\n",
      "Scraping page 151...\n",
      "Scraping page 152...\n",
      "Scraping page 153...\n",
      "Scraping page 154...\n",
      "Scraping page 155...\n",
      "Scraping page 156...\n",
      "Scraping page 157...\n",
      "Scraping page 158...\n",
      "Scraping page 159...\n",
      "Scraping page 160...\n",
      "Scraping page 161...\n",
      "Scraping page 162...\n",
      "Scraping page 163...\n",
      "Scraping page 164...\n",
      "Scraping page 165...\n",
      "Scraping page 166...\n",
      "Scraping page 167...\n",
      "Scraping page 168...\n",
      "Scraping page 169...\n",
      "Scraping page 170...\n",
      "Scraping page 171...\n",
      "Scraping page 172...\n",
      "Scraping page 173...\n",
      "Scraping page 174...\n",
      "Scraping page 175...\n",
      "Scraping page 176...\n",
      "Scraping page 177...\n",
      "Scraping page 178...\n",
      "Scraping page 179...\n",
      "Scraping page 180...\n",
      "Scraping page 181...\n",
      "Scraping page 182...\n",
      "Scraping page 183...\n",
      "Scraping page 184...\n",
      "Scraping page 185...\n",
      "Scraping page 186...\n",
      "Scraping page 187...\n",
      "Scraping page 188...\n",
      "Scraping page 189...\n",
      "Scraping page 190...\n",
      "Scraping page 191...\n",
      "Scraping page 192...\n",
      "Scraping page 193...\n",
      "Scraping page 194...\n",
      "Scraping page 195...\n",
      "Scraping page 196...\n",
      "Scraping page 197...\n",
      "Scraping page 198...\n",
      "Scraping page 199...\n",
      "Scraping page 200...\n",
      "Scraping page 201...\n",
      "Scraping page 202...\n",
      "Scraping page 203...\n",
      "Scraping page 204...\n",
      "Scraping page 205...\n",
      "Scraping page 206...\n",
      "Scraping page 207...\n",
      "Scraping page 208...\n",
      "Scraping page 209...\n",
      "Scraping page 210...\n",
      "Scraping page 211...\n",
      "Scraping page 212...\n",
      "Scraping page 213...\n",
      "Scraping page 214...\n",
      "Scraping page 215...\n",
      "Scraping page 216...\n",
      "Scraping page 217...\n",
      "Scraping page 218...\n",
      "Scraping page 219...\n",
      "Scraping page 220...\n",
      "Scraping page 221...\n",
      "Scraping page 222...\n",
      "Scraping page 223...\n",
      "Scraping page 224...\n",
      "Scraping page 225...\n",
      "Scraping page 226...\n",
      "Scraping page 227...\n",
      "Scraping page 228...\n",
      "Scraping page 229...\n",
      "Scraping page 230...\n",
      "Scraping page 231...\n",
      "Scraping page 232...\n",
      "Scraping page 233...\n",
      "Scraping page 234...\n",
      "Scraping page 235...\n",
      "Scraping page 236...\n",
      "Scraping page 237...\n",
      "Scraping page 238...\n",
      "Scraping page 239...\n",
      "Scraping page 240...\n",
      "Scraping page 241...\n",
      "Scraping page 242...\n",
      "Scraping page 243...\n",
      "Scraping page 244...\n",
      "Scraping page 245...\n",
      "Scraping page 246...\n",
      "Scraping page 247...\n",
      "Scraping page 248...\n",
      "Scraping page 249...\n",
      "Scraping page 250...\n",
      "Scraping page 251...\n",
      "Scraping page 252...\n",
      "Scraping page 253...\n",
      "Scraping page 254...\n",
      "Scraping page 255...\n",
      "Scraping page 256...\n",
      "Scraping page 257...\n",
      "Scraping page 258...\n",
      "Scraping page 259...\n",
      "Scraping page 260...\n",
      "Scraping page 261...\n",
      "Scraping page 262...\n",
      "Scraping page 263...\n",
      "Scraping page 264...\n",
      "Scraping page 265...\n",
      "Scraping page 266...\n",
      "Scraping page 267...\n",
      "Scraping page 268...\n",
      "Scraping page 269...\n",
      "Scraping page 270...\n",
      "Scraping page 271...\n",
      "Scraping page 272...\n",
      "Scraping page 273...\n",
      "Scraping page 274...\n",
      "Scraping page 275...\n",
      "Scraping page 276...\n",
      "Scraping page 277...\n",
      "Scraping page 278...\n",
      "Scraping page 279...\n",
      "Scraping page 280...\n",
      "Scraping page 281...\n",
      "Scraping completed and saved to courses_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Base URL of the courses\n",
    "base_url = \"https://grad.ucalgary.ca/future-students/explore-programs?page={}\"\n",
    "\n",
    "# List to store course names and links\n",
    "courses_data = []\n",
    "\n",
    "# Iterate through all pages (1 to 281)\n",
    "for page_num in range(1, 282):\n",
    "    url = base_url.format(page_num)\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "\n",
    "    # Fetch the page content\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to load page {page_num}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Parse HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all course items on the page\n",
    "    courses = soup.select('li.course-button h3 a')\n",
    "\n",
    "    # Extract course names and links\n",
    "    for course in courses:\n",
    "        course_name = course.get_text(strip=True)\n",
    "        course_link = \"https://grad.ucalgary.ca\" + course.get('href')\n",
    "        courses_data.append({\"Course Name\": course_name, \"Link\": course_link})\n",
    "\n",
    "    # Optional: Pause to prevent overwhelming the server\n",
    "    time.sleep(1)\n",
    "\n",
    "# Save the extracted data to a CSV file\n",
    "df = pd.DataFrame(courses_data)\n",
    "df.to_csv(\"courses_data.csv\", index=False)\n",
    "\n",
    "print(\"Scraping completed and saved to courses_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Main content not found on page 1.\n",
      "Scraping page 2...\n",
      "Main content not found on page 2.\n",
      "Scraping page 3...\n",
      "Main content not found on page 3.\n",
      "Scraping page 4...\n",
      "Main content not found on page 4.\n",
      "Scraping page 5...\n",
      "Main content not found on page 5.\n",
      "Scraping page 6...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-b66078d4e06f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Fetch the HTML content\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Failed to retrieve page {page_num}. Skipping...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m         }\n\u001b[0;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[1;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             response = self._make_request(\n\u001b[0m\u001b[0;32m    790\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[1;31m# Receive the response from the server\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    537\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m         \u001b[1;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m         \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Base URL for pagination\n",
    "base_url = \"https://grad.ucalgary.ca/future-students/explore-programs?page={}\"\n",
    "\n",
    "# List to store course data\n",
    "courses_data = []\n",
    "\n",
    "# Loop through all 281 pages\n",
    "for page_num in range(1, 282):\n",
    "    url = base_url.format(page_num)\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "\n",
    "    # Fetch the HTML content\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page {page_num}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the main content section where all course information is located\n",
    "    main_content = soup.find('div', id='main-content')\n",
    "\n",
    "    if main_content:\n",
    "        # Extract all course names and links\n",
    "        courses = main_content.select('li.course-button h3 a')\n",
    "\n",
    "        for course in courses:\n",
    "            course_name = course.get_text(strip=True)\n",
    "            course_link = \"https://grad.ucalgary.ca\" + course.get('href')\n",
    "            courses_data.append({\"Course Name\": course_name, \"Link\": course_link})\n",
    "    else:\n",
    "        print(f\"Main content not found on page {page_num}.\")\n",
    "\n",
    "    # Optional: Pause to avoid overloading the server\n",
    "    time.sleep(1)\n",
    "\n",
    "# Save the scraped data to a CSV file\n",
    "df = pd.DataFrame(courses_data)\n",
    "df.to_csv(\"courses_data.csv\", index=False)\n",
    "\n",
    "print(\"Scraping completed. Data saved to courses_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "\n",
    "# Set path to WebDriver\n",
    "service = Service(executable_path='/path/to/chromedriver')\n",
    "\n",
    "# Initialize the Chrome browser\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode (no UI)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Open the webpage\n",
    "    driver.get(\"https://calendar.ucalgary.ca/courses?cq=&page=1\")\n",
    "\n",
    "    # Allow some time for the page to load\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Scroll down to load all courses if the page is lazy-loaded\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3)  # Allow time for additional content to load\n",
    "\n",
    "    # Extract course elements and links\n",
    "    courses = driver.find_elements(By.CSS_SELECTOR, \"article#courses-list button.course-button h3\")\n",
    "\n",
    "    for course in courses:\n",
    "        course_name = course.text\n",
    "        parent_button = course.find_element(By.XPATH, \"./ancestor::button\")\n",
    "        course_link = parent_button.get_attribute(\"onclick\")\n",
    "\n",
    "        # Extracting link from 'onclick' if present\n",
    "        if course_link:\n",
    "            course_link = course_link.split(\"'\")[1]\n",
    "\n",
    "        print(f\"Course Name: {course_name}\")\n",
    "        print(f\"Course Link: {course_link}\\n\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
